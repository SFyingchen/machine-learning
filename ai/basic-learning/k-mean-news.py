#!/usr/bin/python3
# -*- coding: utf-8 -*-
# @Author : chen
# @Time : 2024/6/19 15:06
# k-mean-news:  进行新闻 k-means  分类

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer

data = pd.read_csv("./dataset/abcnews.csv", usecols=["headline_text"])
data.to_csv('./dataset/abcnews.csv', index=False, encoding="utf8")

# 获取重复的标题 并让他们相邻  然后最多只获取 8个重复的
data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)
# 删除所有重复的行
data.drop_duplicates('headline_text')

# 使用 tfId 文本向量化
punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', "%"]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)
desc = data['headline_text'].values

# vectorizer = TfidfVectorizer(stop_words=list(stop_words))
# X = vectorizer.fit_transform(desc)
# word_features = vectorizer.get_feature_names_out()

stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'[a-zA-Z\']+')


def tokenize(text):
    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]


# vectorizer2 = TfidfVectorizer(stop_words=list(stop_words), tokenizer=tokenize)
# X2 = vectorizer2.fit_transform(desc)
# word_features2 = vectorizer2.get_feature_names_out()


vectorizer3 = TfidfVectorizer(stop_words=list(stop_words), tokenizer=tokenize, max_features=1000)
X3 = vectorizer3.fit_transform(desc)
words = vectorizer3.get_feature_names_out()
print(len(words))
print(words[:50])
# wcss = []
# for i in range(1, 11):
#     kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
#     kmeans.fit(X3)
#     wcss.append(kmeans.inertia_)
# plt.plot(range(1, 11), wcss)
# plt.title('The Elbow Method')
# plt.xlabel('Number of clusters')
# plt.ylabel('WCSS')
# plt.savefig('elbow.png')
# plt.show()

# n_init(number of iterations for clustering) n_jobs(number of cpu cores to use)
kmeans = KMeans(n_clusters=3, n_init=20)
kmeans.fit(X3)
# We look at 3 the clusters generated by k-means.
common_words = kmeans.cluster_centers_.argsort()[:, -1:-26:-1]
for num, centroid in enumerate(common_words):
    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))
